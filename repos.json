[
    {
        "user": "scrapy",
        "repo": "scrapy",
        "url": "https://github.com/scrapy/scrapy",
        "stars": 48937,
        "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.",
        "topics": [
            "python",
            "crawler",
            "framework",
            "scraping",
            "crawling",
            "web-scraping",
            "hacktoberfest",
            "web-scraping-python"
        ]
    },
    {
        "user": "gocolly",
        "repo": "colly",
        "url": "https://github.com/gocolly/colly",
        "stars": 20921,
        "description": "Elegant Scraper and Crawler Framework for Golang",
        "topics": [
            "go",
            "golang",
            "crawler",
            "scraper",
            "framework",
            "spider",
            "scraping",
            "crawling"
        ]
    },
    {
        "user": "codelucas",
        "repo": "newspaper",
        "url": "https://github.com/codelucas/newspaper",
        "stars": 13190,
        "description": "News, full-text, and article metadata extraction in Python 3. Advanced docs:",
        "topics": [
            "python",
            "crawler",
            "scraper",
            "news",
            "crawling",
            "news-aggregator"
        ]
    },
    {
        "user": "apify",
        "repo": "crawlee",
        "url": "https://github.com/apify/crawlee",
        "stars": 9213,
        "description": "Crawlee—A web scraping and browser automation library for Node.js that helps you build reliable crawlers. Fast.",
        "topics": [
            "nodejs",
            "javascript",
            "npm",
            "crawler",
            "scraper",
            "automation",
            "typescript",
            "web-crawler",
            "headless",
            "scraping",
            "crawling",
            "web-scraping",
            "web-crawling",
            "headless-chrome",
            "apify",
            "puppeteer",
            "playwright"
        ]
    },
    {
        "user": "lorien",
        "repo": "awesome-web-scraping",
        "url": "https://github.com/lorien/awesome-web-scraping",
        "stars": 6017,
        "description": "List of libraries, tools and APIs for web scraping and data processing.",
        "topics": [
            "crawler",
            "spider",
            "scraping",
            "crawling",
            "web-scraping",
            "captcha-recaptcha",
            "webscraping",
            "crawling-framework",
            "scraping-framework",
            "captcha-bypass",
            "scraping-tool",
            "crawling-tool",
            "scraping-python",
            "crawling-python"
        ]
    },
    {
        "user": "MontFerret",
        "repo": "ferret",
        "url": "https://github.com/MontFerret/ferret",
        "stars": 5492,
        "description": "Declarative web scraping",
        "topics": [
            "go",
            "cli",
            "golang",
            "crawler",
            "chrome",
            "data-mining",
            "scraper",
            "library",
            "tool",
            "dsl",
            "scraping",
            "crawling",
            "query-language",
            "scraping-websites",
            "hacktoberfest",
            "cdp",
            "hacktoberfest2021"
        ]
    },
    {
        "user": "yujiosaka",
        "repo": "headless-chrome-crawler",
        "url": "https://github.com/yujiosaka/headless-chrome-crawler",
        "stars": 5417,
        "description": "Distributed crawler powered by Headless Chrome",
        "topics": [
            "jquery",
            "crawler",
            "chrome",
            "scraper",
            "promise",
            "scraping",
            "crawling",
            "chromium",
            "headless-chrome",
            "puppeteer"
        ]
    },
    {
        "user": "go-rod",
        "repo": "rod",
        "url": "https://github.com/go-rod/rod",
        "stars": 4260,
        "description": "A Devtools driver for web automation and scraping",
        "topics": [
            "testing",
            "go",
            "golang",
            "scraper",
            "automation",
            "web",
            "chrome-devtools",
            "headless",
            "devtools",
            "crawling",
            "web-scraping",
            "cdp",
            "chrome-headless",
            "rod",
            "chrome-devtools-protocol",
            "devtools-protocol",
            "gorod"
        ]
    },
    {
        "user": "hakluke",
        "repo": "hakrawler",
        "url": "https://github.com/hakluke/hakrawler",
        "stars": 3993,
        "description": "Simple, fast web crawler designed for easy, quick discovery of endpoints and assets within a web application",
        "topics": [
            "osint",
            "crawling",
            "hacking",
            "pentesting",
            "recon",
            "bugbounty",
            "reconnaissance"
        ]
    },
    {
        "user": "hardkoded",
        "repo": "puppeteer-sharp",
        "url": "https://github.com/hardkoded/puppeteer-sharp",
        "stars": 2862,
        "description": "Headless Chrome .NET API",
        "topics": [
            "crawler",
            "chrome",
            "automation",
            "csharp",
            "crawling",
            "chromium",
            "e2e",
            "webautomation",
            "e2e-testing",
            "puppeteer"
        ]
    },
    {
        "user": "apache",
        "repo": "nutch",
        "url": "https://github.com/apache/nutch",
        "stars": 2691,
        "description": "Apache Nutch is an extensible and scalable web crawler",
        "topics": [
            "java",
            "hadoop",
            "web-crawler",
            "nutch",
            "crawling",
            "apache"
        ]
    },
    {
        "user": "lorien",
        "repo": "grab",
        "url": "https://github.com/lorien/grab",
        "stars": 2323,
        "description": "Web Scraping Framework",
        "topics": [
            "python",
            "crawler",
            "framework",
            "spider",
            "asynchronous",
            "network",
            "python-library",
            "scraping",
            "crawling",
            "http-client",
            "python3",
            "web-scraping",
            "pycurl",
            "urllib3"
        ]
    },
    {
        "user": "transitive-bullshit",
        "repo": "awesome-puppeteer",
        "url": "https://github.com/transitive-bullshit/awesome-puppeteer",
        "stars": 2220,
        "description": "A curated list of awesome puppeteer resources.",
        "topics": [
            "automation",
            "awesome",
            "scraping",
            "crawling",
            "awesome-list",
            "headless-chrome",
            "puppeteer"
        ]
    },
    {
        "user": "zorlan",
        "repo": "skycaiji",
        "url": "https://github.com/zorlan/skycaiji",
        "stars": 1766,
        "description": "蓝天采集器是一款开源免费的爬虫系统，仅需点选编辑规则即可采集数据，可运行在本地、虚拟主机或云服务器中，几乎能采集所有类型的网页，无缝对接各类CMS建站程序，免登录实时发布数据，全自动无需人工干预！是网页大数据采集软件中完全跨平台的云端爬虫系统",
        "topics": [
            "php",
            "crawler",
            "spider",
            "crawling",
            "webcrawler"
        ]
    },
    {
        "user": "roach-php",
        "repo": "core",
        "url": "https://github.com/roach-php/core",
        "stars": 1263,
        "description": "The complete web scraping toolkit for PHP.",
        "topics": [
            "php",
            "crawling",
            "web-scraping"
        ]
    },
    {
        "user": "edoardottt",
        "repo": "cariddi",
        "url": "https://github.com/edoardottt/cariddi",
        "stars": 1143,
        "description": "Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more",
        "topics": [
            "go",
            "golang",
            "security",
            "crawler",
            "scraper",
            "osint",
            "crawling",
            "penetration-testing",
            "infosec",
            "pentesting",
            "recon",
            "bugbounty",
            "hacktoberfest",
            "security-tools",
            "endpoints",
            "reconnaissance",
            "secret-keys",
            "endpoint-discovery",
            "secrets-detection",
            "asset-finder"
        ]
    },
    {
        "user": "lorey",
        "repo": "mlscraper",
        "url": "https://github.com/lorey/mlscraper",
        "stars": 1073,
        "description": "🤖 Scrape data from HTML websites automatically by just providing examples",
        "topics": [
            "html",
            "crawler",
            "machine-learning",
            "scraper",
            "scraping",
            "crawling",
            "crawler-python",
            "extraction-engine"
        ]
    },
    {
        "user": "needleworm",
        "repo": "bhban_rpa",
        "url": "https://github.com/needleworm/bhban_rpa",
        "stars": 883,
        "description": "<6개월 치 업무를 하루 만에 끝내는 업무 자동화(생능출판사, 2020)>의 예제 코드입니다. 파이썬을 한 번도 배워본 적 없는 분들을 위한 예제이며, 엑셀부터 디자인, 매크로, 크롤링까지 업무 자동화와 관련된 다양한 분야 예제가 제공됩니다.",
        "topics": [
            "python",
            "education",
            "design",
            "automation",
            "crawling",
            "rpa"
        ]
    },
    {
        "user": "NateScarlet",
        "repo": "holiday-cn",
        "url": "https://github.com/NateScarlet/holiday-cn",
        "stars": 863,
        "description": "📅🇨🇳中国法定节假日数据 自动每日抓取国务院公告",
        "topics": [
            "data",
            "natural-language-processing",
            "crawling",
            "china",
            "holiday"
        ]
    },
    {
        "user": "clemfromspace",
        "repo": "scrapy-selenium",
        "url": "https://github.com/clemfromspace/scrapy-selenium",
        "stars": 842,
        "description": "Scrapy middleware to handle javascript pages using selenium",
        "topics": [
            "crawling",
            "selenium",
            "scrapy"
        ]
    },
    {
        "user": "iawia002",
        "repo": "Lulu",
        "url": "https://github.com/iawia002/Lulu",
        "stars": 812,
        "description": "[Unmaintained] A simple and clean video/music/image downloader 👾",
        "topics": [
            "python",
            "crawler",
            "scraper",
            "downloader",
            "video",
            "scraping",
            "crawling",
            "python3"
        ]
    },
    {
        "user": "scrapinghub",
        "repo": "scrapyrt",
        "url": "https://github.com/scrapinghub/scrapyrt",
        "stars": 791,
        "description": "HTTP API for Scrapy spiders",
        "topics": [
            "python",
            "crawler",
            "scraper",
            "crawling",
            "twisted",
            "scrapy",
            "webcrawler",
            "hacktoberfest",
            "webcrawling",
            "hacktoberfest2021"
        ]
    },
    {
        "user": "elixir-crawly",
        "repo": "crawly",
        "url": "https://github.com/elixir-crawly/crawly",
        "stars": 768,
        "description": "Crawly, a high-level web crawling & scraping framework for Elixir.",
        "topics": [
            "crawler",
            "scraper",
            "erlang",
            "elixir",
            "spider",
            "scraping",
            "crawling",
            "extract-data",
            "scraping-websites"
        ]
    },
    {
        "user": "MorvanZhou",
        "repo": "easy-scraping-tutorial",
        "url": "https://github.com/MorvanZhou/easy-scraping-tutorial",
        "stars": 732,
        "description": "Simple but useful Python web scraping tutorial code.",
        "topics": [
            "crawler",
            "regex",
            "scraping",
            "crawling",
            "requests",
            "asyncio",
            "scrapy",
            "beautifulsoup",
            "distributed-scraper",
            "urllib"
        ]
    },
    {
        "user": "slotix",
        "repo": "dataflowkit",
        "url": "https://github.com/slotix/dataflowkit",
        "stars": 620,
        "description": "Extract structured data from web sites. Web sites scraping.",
        "topics": [
            "go",
            "golang",
            "scraper",
            "headless",
            "scraping",
            "crawling",
            "golang-library",
            "extract-data",
            "scraping-websites",
            "cdp",
            "chrome-fetcher"
        ]
    },
    {
        "user": "essandess",
        "repo": "isp-data-pollution",
        "url": "https://github.com/essandess/isp-data-pollution",
        "stars": 541,
        "description": "ISP Data Pollution to Protect Private Browsing History with Obfuscation",
        "topics": [
            "data",
            "privacy",
            "obfuscation",
            "web",
            "crawling",
            "data-analytics",
            "privacy-enhancing-technologies"
        ]
    },
    {
        "user": "mishakorzik",
        "repo": "AdminHack",
        "url": "https://github.com/mishakorzik/AdminHack",
        "stars": 519,
        "description": "today we will hack the admin panel of the site.",
        "topics": [
            "linux",
            "website",
            "crawling",
            "cpanel",
            "termux",
            "kali-linux",
            "admin-finder",
            "admin-panel",
            "hacking-tool",
            "cpanl-finder",
            "directory-bruteforce",
            "website-hacking",
            "termux-tool",
            "termux-hacking",
            "website-hacking-methods",
            "websitehacking",
            "allhackingtools",
            "admin-hack",
            "admin-website-hack"
        ]
    },
    {
        "user": "bluet",
        "repo": "proxybroker2",
        "url": "https://github.com/bluet/proxybroker2",
        "stars": 515,
        "description": "The New (auto rotate) Proxy [Finder | Checker | Server]. HTTP(S) & SOCKS 🎭",
        "topics": [
            "crawler",
            "privacy",
            "proxy",
            "crawling",
            "proxy-server",
            "http-proxy",
            "https-proxy",
            "socks",
            "proxies",
            "anonymity",
            "anonymous",
            "proxypool",
            "proxy-list",
            "hacktoberfest",
            "proxychains",
            "proxy-checker"
        ]
    },
    {
        "user": "crawljax",
        "repo": "crawljax",
        "url": "https://github.com/crawljax/crawljax",
        "stars": 496,
        "description": "Crawljax",
        "topics": [
            "javascript",
            "crawler",
            "dom",
            "dynamic",
            "crawling",
            "test-generation",
            "web-testing",
            "web-analysis",
            "event-driven-crawling"
        ]
    },
    {
        "user": "scrapinghub",
        "repo": "spidermon",
        "url": "https://github.com/scrapinghub/spidermon",
        "stars": 485,
        "description": "Scrapy Extension for monitoring spiders execution.",
        "topics": [
            "testing",
            "monitoring",
            "scraping",
            "crawling",
            "spiders",
            "hacktoberfest",
            "monitoring-tool",
            "scrapinghub"
        ]
    },
    {
        "user": "zhuyingda",
        "repo": "webster",
        "url": "https://github.com/zhuyingda/webster",
        "stars": 464,
        "description": "a reliable high-level web crawling & scraping framework for Node.js.",
        "topics": [
            "nodejs",
            "javascript",
            "crawler",
            "spider",
            "javascript-framework",
            "crawling",
            "chromium",
            "automation-ui",
            "nodejs-framework",
            "automation-test",
            "headless-chrome",
            "scraping-framework",
            "puppeteer"
        ]
    },
    {
        "user": "josephlimtech",
        "repo": "linkedin-profile-scraper",
        "url": "https://github.com/josephlimtech/linkedin-profile-scraper",
        "stars": 389,
        "description": "🕵️‍♂️ LinkedIn profile scraper returning structured profile data in JSON.",
        "topics": [
            "nodejs",
            "json",
            "crawler",
            "scraper",
            "spider",
            "linkedin",
            "scraping",
            "crawling",
            "expressjs",
            "linkedin-profile",
            "scrapers",
            "scraping-websites",
            "linkedin-bot",
            "website-scraper",
            "profile-data",
            "linkedin-scraper",
            "linkedin-crawler",
            "puppeteer",
            "linkedin-scraping",
            "linkedin-profile-scraper"
        ]
    },
    {
        "user": "Florents-Tselai",
        "repo": "WarcDB",
        "url": "https://github.com/Florents-Tselai/WarcDB",
        "stars": 377,
        "description": "WarcDB: Web crawl data as SQLite databases.",
        "topics": [
            "cli",
            "database",
            "sqlite",
            "crawling",
            "warc",
            "web-archiving",
            "web-data"
        ]
    },
    {
        "user": "mhmdiaa",
        "repo": "second-order",
        "url": "https://github.com/mhmdiaa/second-order",
        "stars": 338,
        "description": "Second-order subdomain takeover scanner",
        "topics": [
            "security",
            "crawler",
            "mapping",
            "crawling",
            "wordlist",
            "penetration-testing",
            "infosec",
            "pentesting",
            "recon",
            "wordlist-generator",
            "security-tools",
            "web-application-security",
            "reconnaissance",
            "penetration-testing-tools"
        ]
    },
    {
        "user": "rivermont",
        "repo": "spidy",
        "url": "https://github.com/rivermont/spidy",
        "stars": 315,
        "description": "The simple, easy to use command line web crawler.",
        "topics": [
            "python",
            "crawler",
            "web-crawler",
            "crawling",
            "python3",
            "web-spider"
        ]
    },
    {
        "user": "l4rm4nd",
        "repo": "LinkedInDumper",
        "url": "https://github.com/l4rm4nd/LinkedInDumper",
        "stars": 310,
        "description": "Python 3 script to dump/scrape/extract company employees from LinkedIn API",
        "topics": [
            "osint",
            "spider",
            "linkedin",
            "scraping",
            "crawling",
            "python3",
            "employees",
            "extracting"
        ]
    },
    {
        "user": "stopstalk",
        "repo": "stopstalk-deployment",
        "url": "https://github.com/stopstalk/stopstalk-deployment",
        "stars": 306,
        "description": "Stop stalking and start StopStalking 😉",
        "topics": [
            "python",
            "aws",
            "crawling",
            "codechef",
            "spoj",
            "uva",
            "competitive-programming",
            "hackerrank",
            "codeforces",
            "web2py",
            "materializecss",
            "hackerearth",
            "atcoder",
            "programming-contests",
            "hacktoberfest",
            "timus",
            "stopstalk"
        ]
    },
    {
        "user": "Malwarize",
        "repo": "webpalm",
        "url": "https://github.com/Malwarize/webpalm",
        "stars": 297,
        "description": "WebPalm is a powerful command-line tool for website mapping and web scraping. With its recursive approach, it can generate a complete tree of all webpages and their links on a website. It can also extract data from the body of each page using regular expressions, making it an ideal tool for web scraping and data extraction.",
        "topics": [
            "go",
            "golang",
            "crawler",
            "osint",
            "spider",
            "hack",
            "tool",
            "crawling",
            "redteam"
        ]
    },
    {
        "user": "alephdata",
        "repo": "memorious",
        "url": "https://github.com/alephdata/memorious",
        "stars": 297,
        "description": "Lightweight web scraping toolkit for documents and structured data.",
        "topics": [
            "scraping",
            "crawling",
            "scraping-framework"
        ]
    },
    {
        "user": "infinitbyte",
        "repo": "gopa",
        "url": "https://github.com/infinitbyte/gopa",
        "stars": 296,
        "description": "GOPA, a spider written in Golang, for Elasticsearch. DEMO: http://index.elasticsearch.cn",
        "topics": [
            "lightweight",
            "elasticsearch",
            "crawler",
            "spider",
            "web-crawler",
            "scraping",
            "crawling",
            "web-scraping",
            "web-spider"
        ]
    }
]