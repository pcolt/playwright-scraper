"user","repo","url","stars","description","topics"
"scrapy","scrapy","https://github.com/scrapy/scrapy",48937,"Scrapy, a fast high-level web crawling & scraping framework for Python.","[""python"",""crawler"",""framework"",""scraping"",""crawling"",""web-scraping"",""hacktoberfest"",""web-scraping-python""]"
"gocolly","colly","https://github.com/gocolly/colly",20920,"Elegant Scraper and Crawler Framework for Golang","[""go"",""golang"",""crawler"",""scraper"",""framework"",""spider"",""scraping"",""crawling""]"
"codelucas","newspaper","https://github.com/codelucas/newspaper",13190,"News, full-text, and article metadata extraction in Python 3. Advanced docs:","[""python"",""crawler"",""scraper"",""news"",""crawling"",""news-aggregator""]"
"apify","crawlee","https://github.com/apify/crawlee",9213,"Crawlee—A web scraping and browser automation library for Node.js that helps you build reliable crawlers. Fast.","[""nodejs"",""javascript"",""npm"",""crawler"",""scraper"",""automation"",""typescript"",""web-crawler"",""headless"",""scraping"",""crawling"",""web-scraping"",""web-crawling"",""headless-chrome"",""apify"",""puppeteer"",""playwright""]"
"lorien","awesome-web-scraping","https://github.com/lorien/awesome-web-scraping",6017,"List of libraries, tools and APIs for web scraping and data processing.","[""crawler"",""spider"",""scraping"",""crawling"",""web-scraping"",""captcha-recaptcha"",""webscraping"",""crawling-framework"",""scraping-framework"",""captcha-bypass"",""scraping-tool"",""crawling-tool"",""scraping-python"",""crawling-python""]"
"MontFerret","ferret","https://github.com/MontFerret/ferret",5492,"Declarative web scraping","[""go"",""cli"",""golang"",""crawler"",""chrome"",""data-mining"",""scraper"",""library"",""tool"",""dsl"",""scraping"",""crawling"",""query-language"",""scraping-websites"",""hacktoberfest"",""cdp"",""hacktoberfest2021""]"
"yujiosaka","headless-chrome-crawler","https://github.com/yujiosaka/headless-chrome-crawler",5417,"Distributed crawler powered by Headless Chrome","[""jquery"",""crawler"",""chrome"",""scraper"",""promise"",""scraping"",""crawling"",""chromium"",""headless-chrome"",""puppeteer""]"
"go-rod","rod","https://github.com/go-rod/rod",4260,"A Devtools driver for web automation and scraping","[""testing"",""go"",""golang"",""scraper"",""automation"",""web"",""chrome-devtools"",""headless"",""devtools"",""crawling"",""web-scraping"",""cdp"",""chrome-headless"",""rod"",""chrome-devtools-protocol"",""devtools-protocol"",""gorod""]"
"hakluke","hakrawler","https://github.com/hakluke/hakrawler",3993,"Simple, fast web crawler designed for easy, quick discovery of endpoints and assets within a web application","[""osint"",""crawling"",""hacking"",""pentesting"",""recon"",""bugbounty"",""reconnaissance""]"
"hardkoded","puppeteer-sharp","https://github.com/hardkoded/puppeteer-sharp",2862,"Headless Chrome .NET API","[""crawler"",""chrome"",""automation"",""csharp"",""crawling"",""chromium"",""e2e"",""webautomation"",""e2e-testing"",""puppeteer""]"
"apache","nutch","https://github.com/apache/nutch",2691,"Apache Nutch is an extensible and scalable web crawler","[""java"",""hadoop"",""web-crawler"",""nutch"",""crawling"",""apache""]"
"lorien","grab","https://github.com/lorien/grab",2323,"Web Scraping Framework","[""python"",""crawler"",""framework"",""spider"",""asynchronous"",""network"",""python-library"",""scraping"",""crawling"",""http-client"",""python3"",""web-scraping"",""pycurl"",""urllib3""]"
"transitive-bullshit","awesome-puppeteer","https://github.com/transitive-bullshit/awesome-puppeteer",2219,"A curated list of awesome puppeteer resources.","[""automation"",""awesome"",""scraping"",""crawling"",""awesome-list"",""headless-chrome"",""puppeteer""]"
"zorlan","skycaiji","https://github.com/zorlan/skycaiji",1766,"蓝天采集器是一款开源免费的爬虫系统，仅需点选编辑规则即可采集数据，可运行在本地、虚拟主机或云服务器中，几乎能采集所有类型的网页，无缝对接各类CMS建站程序，免登录实时发布数据，全自动无需人工干预！是网页大数据采集软件中完全跨平台的云端爬虫系统","[""php"",""crawler"",""spider"",""crawling"",""webcrawler""]"
"roach-php","core","https://github.com/roach-php/core",1263,"The complete web scraping toolkit for PHP.","[""php"",""crawling"",""web-scraping""]"
"edoardottt","cariddi","https://github.com/edoardottt/cariddi",1143,"Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more","[""go"",""golang"",""security"",""crawler"",""scraper"",""osint"",""crawling"",""penetration-testing"",""infosec"",""pentesting"",""recon"",""bugbounty"",""hacktoberfest"",""security-tools"",""endpoints"",""reconnaissance"",""secret-keys"",""endpoint-discovery"",""secrets-detection"",""asset-finder""]"
"lorey","mlscraper","https://github.com/lorey/mlscraper",1073,"🤖 Scrape data from HTML websites automatically by just providing examples","[""html"",""crawler"",""machine-learning"",""scraper"",""scraping"",""crawling"",""crawler-python"",""extraction-engine""]"
"needleworm","bhban_rpa","https://github.com/needleworm/bhban_rpa",883,"<6개월 치 업무를 하루 만에 끝내는 업무 자동화(생능출판사, 2020)>의 예제 코드입니다. 파이썬을 한 번도 배워본 적 없는 분들을 위한 예제이며, 엑셀부터 디자인, 매크로, 크롤링까지 업무 자동화와 관련된 다양한 분야 예제가 제공됩니다.","[""python"",""education"",""design"",""automation"",""crawling"",""rpa""]"
"NateScarlet","holiday-cn","https://github.com/NateScarlet/holiday-cn",863,"📅🇨🇳中国法定节假日数据 自动每日抓取国务院公告","[""data"",""natural-language-processing"",""crawling"",""china"",""holiday""]"
"clemfromspace","scrapy-selenium","https://github.com/clemfromspace/scrapy-selenium",842,"Scrapy middleware to handle javascript pages using selenium","[""crawling"",""selenium"",""scrapy""]"
"iawia002","Lulu","https://github.com/iawia002/Lulu",812,"[Unmaintained] A simple and clean video/music/image downloader 👾","[""python"",""crawler"",""scraper"",""downloader"",""video"",""scraping"",""crawling"",""python3""]"
"scrapinghub","scrapyrt","https://github.com/scrapinghub/scrapyrt",791,"HTTP API for Scrapy spiders","[""python"",""crawler"",""scraper"",""crawling"",""twisted"",""scrapy"",""webcrawler"",""hacktoberfest"",""webcrawling"",""hacktoberfest2021""]"
"elixir-crawly","crawly","https://github.com/elixir-crawly/crawly",768,"Crawly, a high-level web crawling & scraping framework for Elixir.","[""crawler"",""scraper"",""erlang"",""elixir"",""spider"",""scraping"",""crawling"",""extract-data"",""scraping-websites""]"
"MorvanZhou","easy-scraping-tutorial","https://github.com/MorvanZhou/easy-scraping-tutorial",732,"Simple but useful Python web scraping tutorial code.","[""crawler"",""regex"",""scraping"",""crawling"",""requests"",""asyncio"",""scrapy"",""beautifulsoup"",""distributed-scraper"",""urllib""]"
"slotix","dataflowkit","https://github.com/slotix/dataflowkit",620,"Extract structured data from web sites. Web sites scraping.","[""go"",""golang"",""scraper"",""headless"",""scraping"",""crawling"",""golang-library"",""extract-data"",""scraping-websites"",""cdp"",""chrome-fetcher""]"
"essandess","isp-data-pollution","https://github.com/essandess/isp-data-pollution",541,"ISP Data Pollution to Protect Private Browsing History with Obfuscation","[""data"",""privacy"",""obfuscation"",""web"",""crawling"",""data-analytics"",""privacy-enhancing-technologies""]"
"mishakorzik","AdminHack","https://github.com/mishakorzik/AdminHack",519,"today we will hack the admin panel of the site.","[""linux"",""website"",""crawling"",""cpanel"",""termux"",""kali-linux"",""admin-finder"",""admin-panel"",""hacking-tool"",""cpanl-finder"",""directory-bruteforce"",""website-hacking"",""termux-tool"",""termux-hacking"",""website-hacking-methods"",""websitehacking"",""allhackingtools"",""admin-hack"",""admin-website-hack""]"
"bluet","proxybroker2","https://github.com/bluet/proxybroker2",515,"The New (auto rotate) Proxy [Finder | Checker | Server]. HTTP(S) & SOCKS 🎭","[""crawler"",""privacy"",""proxy"",""crawling"",""proxy-server"",""http-proxy"",""https-proxy"",""socks"",""proxies"",""anonymity"",""anonymous"",""proxypool"",""proxy-list"",""hacktoberfest"",""proxychains"",""proxy-checker""]"
"crawljax","crawljax","https://github.com/crawljax/crawljax",496,"Crawljax","[""javascript"",""crawler"",""dom"",""dynamic"",""crawling"",""test-generation"",""web-testing"",""web-analysis"",""event-driven-crawling""]"
"scrapinghub","spidermon","https://github.com/scrapinghub/spidermon",485,"Scrapy Extension for monitoring spiders execution.","[""testing"",""monitoring"",""scraping"",""crawling"",""spiders"",""hacktoberfest"",""monitoring-tool"",""scrapinghub""]"
"zhuyingda","webster","https://github.com/zhuyingda/webster",464,"a reliable high-level web crawling & scraping framework for Node.js.","[""nodejs"",""javascript"",""crawler"",""spider"",""javascript-framework"",""crawling"",""chromium"",""automation-ui"",""nodejs-framework"",""automation-test"",""headless-chrome"",""scraping-framework"",""puppeteer""]"
"josephlimtech","linkedin-profile-scraper","https://github.com/josephlimtech/linkedin-profile-scraper",389,"🕵️‍♂️ LinkedIn profile scraper returning structured profile data in JSON.","[""nodejs"",""json"",""crawler"",""scraper"",""spider"",""linkedin"",""scraping"",""crawling"",""expressjs"",""linkedin-profile"",""scrapers"",""scraping-websites"",""linkedin-bot"",""website-scraper"",""profile-data"",""linkedin-scraper"",""linkedin-crawler"",""puppeteer"",""linkedin-scraping"",""linkedin-profile-scraper""]"
"Florents-Tselai","WarcDB","https://github.com/Florents-Tselai/WarcDB",377,"WarcDB: Web crawl data as SQLite databases.","[""cli"",""database"",""sqlite"",""crawling"",""warc"",""web-archiving"",""web-data""]"
"mhmdiaa","second-order","https://github.com/mhmdiaa/second-order",338,"Second-order subdomain takeover scanner","[""security"",""crawler"",""mapping"",""crawling"",""wordlist"",""penetration-testing"",""infosec"",""pentesting"",""recon"",""wordlist-generator"",""security-tools"",""web-application-security"",""reconnaissance"",""penetration-testing-tools""]"
"rivermont","spidy","https://github.com/rivermont/spidy",315,"The simple, easy to use command line web crawler.","[""python"",""crawler"",""web-crawler"",""crawling"",""python3"",""web-spider""]"
"l4rm4nd","LinkedInDumper","https://github.com/l4rm4nd/LinkedInDumper",310,"Python 3 script to dump/scrape/extract company employees from LinkedIn API","[""osint"",""spider"",""linkedin"",""scraping"",""crawling"",""python3"",""employees"",""extracting""]"
"stopstalk","stopstalk-deployment","https://github.com/stopstalk/stopstalk-deployment",306,"Stop stalking and start StopStalking 😉","[""python"",""aws"",""crawling"",""codechef"",""spoj"",""uva"",""competitive-programming"",""hackerrank"",""codeforces"",""web2py"",""materializecss"",""hackerearth"",""atcoder"",""programming-contests"",""hacktoberfest"",""timus"",""stopstalk""]"
"Malwarize","webpalm","https://github.com/Malwarize/webpalm",297,"WebPalm is a powerful command-line tool for website mapping and web scraping. With its recursive approach, it can generate a complete tree of all webpages and their links on a website. It can also extract data from the body of each page using regular expressions, making it an ideal tool for web scraping and data extraction.","[""go"",""golang"",""crawler"",""osint"",""spider"",""hack"",""tool"",""crawling"",""redteam""]"
"alephdata","memorious","https://github.com/alephdata/memorious",297,"Lightweight web scraping toolkit for documents and structured data.","[""scraping"",""crawling"",""scraping-framework""]"
"infinitbyte","gopa","https://github.com/infinitbyte/gopa",296,"GOPA, a spider written in Golang, for Elasticsearch. DEMO: http://index.elasticsearch.cn","[""lightweight"",""elasticsearch"",""crawler"",""spider"",""web-crawler"",""scraping"",""crawling"",""web-scraping"",""web-spider""]"